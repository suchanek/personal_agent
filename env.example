# Personal AI Agent Environment Variables
# Copy this file to .env and fill in your actual API keys

# --- Core Directories ---
# These paths are essential for the agent's operation.
# ROOT_DIR: The root directory for filesystem access.
# HOME_DIR: The user's home directory.
# DATA_DIR: The main directory for storing data like vector databases.
# REPO_DIR: A directory for cloning or managing code repositories.
ROOT_DIR="/Users/your_username"
HOME_DIR="/Users/your_username"
DATA_DIR="/Users/your_username/data"
REPO_DIR="/Users/your_username/repos"

# --- API Keys (Secrets) ---
# Fill these in with your personal keys.
# GitHub Personal Access Token: https://github.com/settings/tokens
GITHUB_PERSONAL_ACCESS_TOKEN=""

# Brave Search API Key: https://api.search.brave.com/app/keys
BRAVE_API_KEY=""

# ModelsLab API Key: https://modelslab.com/
MODELS_LAB_API_KEY=""

# ElevenLabs API Key: https://elevenlabs.io/
ELEVEN_LABS_API_KEY=""

# Giphy API Key: https://developers.giphy.com/
GIPHY_API_KEY=""


# --- Core Agent Configuration ---
WEAVIATE_URL="http://localhost:8080"
REMOTE_OLLAMA_URL="http://<your remote server>:11434"
OLLAMA_URL="http://localhost:11434"
# hook into the LightRag container
OLLAMA_DOCKER_URL="http://host.docker.internal:11434"

USE_WEAVIATE="False"
USE_MCP="True"
LOG_LEVEL="INFO"
# LLM Model configuration - use actual model names from your Ollama instance
LLM_MODEL="qwen3:8b"

# --- Storage Configuration ---
# Storage backend: "weaviate" or "agno"
STORAGE_BACKEND="agno"
AGNO_STORAGE_DIR="${DATA_DIR}/${STORAGE_BACKEND}"
AGNO_KNOWLEDGE_DIR="${DATA_DIR}/knowledge"

# --- User Configuration ---
USER_ID="user"

# --- MODEL CONTEXT SIZE OVERRIDES ---
# Override context sizes for specific models (optional)
# Format: MODEL_NAME_CTX_SIZE (replace : with _ and . with _)
# Examples:
# QWEN3_1_7B_CTX_SIZE=16384
# LLAMA3_1_8B_INSTRUCT_Q8_0_CTX_SIZE=65536
# DEFAULT_MODEL_CTX_SIZE=8192
# --- Rate Limiting ---
DUCKDUCKGO_SEARCH_DELAY="3.0"
DUCKDUCKGO_MAX_RETRIES="3"
DUCKDUCKGO_RETRY_DELAY="10.0"
DEFAULT_API_DELAY="1.0"
DEFAULT_MAX_RETRIES="3"
DEFAULT_RETRY_DELAY="5.0"

# --- LightRAG Configuration ---
LLM_TYPE="ollama"
LLM_MODEL_LIGHRAG="${LLM_MODEL}"

EMBEDDING_BINDING_API_KEY=""
# Embedding model (also via Ollama)
EMBEDDING_TYPE="ollama"
EMBEDDING_MODEL="nomic-embed-text"
EMBEDDING_BINDING="ollama"
EMBEDDING_BINDING_HOST="${OLLAMA_DOCKER_URL}"
EMBEDDING_DIM="768"

# Vector DB backend for LightRAG
VECTOR_STORE="lancedb://${DATA_DIR}/lightrag/lancedb"

# Knowledge graph storage backend for LightRAG
GRAPH_STORE="sqlite://${DATA_DIR}/lightrag/lightrag_graph.db"

# Optional LightRAG REST server configuration
LIGHTRAG_SERVER="http://localhost:9621/webui/"
SERVER_PORT="8000"
SERVER_HOST="0.0.0.0"
LLM_BINDING_API_KEY=""
WORKERS="4"
MAX_ASYNC_LLM="2"
MAX_PARALLEL_INSERT="2"

LLM_BINDING="ollama"
LLM_BINDING_HOST="${OLLAMA_DOCKER_URL}"
MAX_TOKENS="8192"

# --- Timeout Settings ---
# These are aggressive timeouts for processing large documents.
# Adjust them based on your hardware and network speed.
OLLAMA_KEEP_ALIVE="7200"
OLLAMA_NUM_PARALLEL="1"
OLLAMA_REQUEST_TIMEOUT="7200"
OLLAMA_CONNECT_TIMEOUT="600"
OLLAMA_CLIENT_TIMEOUT="7200"
OLLAMA_HTTPX_TIMEOUT="7200"
TIMEOUT="7200"
REQUEST_TIMEOUT="7200"
CLIENT_TIMEOUT="7200"
LLM_TIMEOUT="7200"
EMBEDDING_TIMEOUT="3600"
PDF_CHUNK_SIZE="2048"
HTTP_TIMEOUT="7200"
CONNECTION_TIMEOUT="600"
READ_TIMEOUT="7200"
WRITE_TIMEOUT="600"
POOL_TIMEOUT="600"
HTTPX_TIMEOUT="7200"
HTTPX_CONNECT_TIMEOUT="600"
HTTPX_READ_TIMEOUT="7200"
HTTPX_WRITE_TIMEOUT="600"
HTTPX_POOL_TIMEOUT="600"
