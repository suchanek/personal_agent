# Personal AI Agent Environment Variables
# Copy this file to .env and fill in your actual API keys

### Server Configuration
LIGHTRAG_MEMORY_PORT=9622
WEBUI_TITLE="Personal Agent Memory Base"
WEBUI_DESCRIPTION="P.Agent Knowledge-Graph RAG"

# Directories for the Personal AI Agent
ROOT_DIR=/                     # Full filesystem access
HOME_DIR=/Users/egs           # User's home directory  
DATA_DIR=/Users/Shared/personal_agent_data     # Data directory for vector database
# USER CONFIGURATION
USER_ID=charlie

#OLLAMA_URL=http://tesla.local:11434
OLLAMA_URL=http://localhost:11434
OLLAMA_DOCKER_URL=http://host.docker.internal:11434

# LLM Model configuration - use actual model names, not dictionary references
#LLM_MODEL=llama3.1:8b-instruct-q8_0
LLM_MODEL=qwen3:1.7b
EMBEDDING_MODEL=nomic-embed-text

# Agno Storage Configuration
# Storage backend: "weaviate" or "agno"
STORAGE_BACKEND=agno
AGNO_STORAGE_DIR=${DATA_DIR}/${STORAGE_BACKEND}/${USER_ID}

# Knowledge directory for Agno
AGNO_KNOWLEDGE_DIR=${DATA_DIR}/${STORAGE_BACKEND}/${USER_ID}/knowledge


# MODEL CONTEXT SIZE OVERRIDES
# Override context sizes for specific models (optional)
# Format: MODEL_NAME_CTX_SIZE (replace : with _ and . with _)
# Examples:
QWEN3_1_7B_CTX_SIZE=16384
QWEN3_4B_CTX_SIZE=16384

# LLAMA3_1_8B_INSTRUCT_Q8_0_CTX_SIZE=65536
DEFAULT_MODEL_CTX_SIZE=8192


# Embedding model (also via Ollama)
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BINDING=ollama
EMBEDDING_BINDING_HOST=http://host.docker.internal:11434
EMBEDDING_DIM=768

# LLM Binding Configuration
LLM_BINDING_HOST=http://host.docker.internal:11434
# EMBEDDING_BINDING_API_KEY=your_api_key
# Optional LightRAG REST server configuration

# GUI Endpoint
LIGHTRAG_MEMORY_SERVER=http://localhost:9622
LIGHTRAG_MEMORY_SERVER_URL=http://localhost:9622/webui

# LLM_BINDING_API_KEY=your_api_key
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=7200
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0
### Max concurrency requests of LLM
MAX_ASYNC=1

# LIGHTRAG TIMEOUT SETTINGS FOR PDF PROCESSING
LLM_TIMEOUT=7200          # 120 minutes for LLM processing (increased for remote Ollama)
EMBEDDING_TIMEOUT=3600    # 60 minutes for embedding processing (increased for remote)
PDF_CHUNK_SIZE=512        # Changed from 2048 to 512 to match docker-compose.yml
HTTP_TIMEOUT=7200         # 120 minutes for HTTP client timeout
CONNECTION_TIMEOUT=600    # 10 minutes for initial connection timeout
READ_TIMEOUT=7200         # 120 minutes for read operations
WRITE_TIMEOUT=600         # 10 minutes for write operations
POOL_TIMEOUT=600          # 10 minutes for connection pool timeout

# HTTPX CLIENT TIMEOUT SETTINGS (for Ollama client)
HTTPX_TIMEOUT=7200        # 120 minutes for httpx client operations
HTTPX_CONNECT_TIMEOUT=600 # 10 minutes for httpx connection timeout
HTTPX_READ_TIMEOUT=7200   # 120 minutes for httpx read timeout
HTTPX_WRITE_TIMEOUT=600   # 10 minutes for httpx write timeout
HTTPX_POOL_TIMEOUT=600    # 10 minutes for httpx pool timeout

# CHUNK SETTINGS (from docker-compose.yml)
CHUNK_SIZE=512
CHUNK_OVERLAP_SIZE=100

# WORKER SETTINGS (from docker-compose.yml)
WORKERS=1
MAX_PARALLEL_INSERT=1

# ADDITIONAL OLLAMA SETTINGS (from docker-compose.yml)
OLLAMA_TIMEOUT=7200
OLLAMA_KEEP_ALIVE=3600
OLLAMA_NUM_PREDICT=16384
OLLAMA_TEMPERATURE=0.1
OLLAMA_REQUEST_TIMEOUT=14400

# ADDITIONAL TIMEOUT SETTINGS (from docker-compose.yml)
REQUESTS_TIMEOUT=14400
URLLIB3_TIMEOUT=14400
AIOHTTP_TIMEOUT=14400
SOCKET_TIMEOUT=14400

# TCP SETTINGS (from docker-compose.yml)
TCP_KEEPALIVE=true
TCP_KEEPALIVE_IDLE=600
TCP_KEEPALIVE_INTERVAL=60
TCP_KEEPALIVE_COUNT=9
