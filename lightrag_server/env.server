# Personal AI Agent Environment Variables
### Server Configuration
HOST=0.0.0.0
LIGHTRAG_SERVER_PORT=9622
WEBUI_TITLE='Personal Agent KnowledgeBase'
WEBUI_DESCRIPTION="Personal Agent Knowledge-Graph RAG"
OLLAMA_EMULATING_MODEL_TAG=latest
WORKERS=1

# Directories for the Personal AI Agent
ROOT_DIR=/                     # Full filesystem access
HOME_DIR=/Users/egs           # User's home directory
DATA_DIR=/Users/Shared/personal_agent_data     # Data directory for vector database

#OLLAMA_URL=http://tesla.local:11434
OLLAMA_URL=http://localhost:11434
OLLAMA_DOCKER_URL=http://host.docker.internal:11434


# LLM Model configuration - use actual model names, not dictionary references
# Upgraded to more capable model for better PDF processing

LLM_MODEL=qwen2.5:latest
#LLM_MODEL=llama3.1:8b-instruct-q8_0
#LLM_MODEL=qwen3:1.7b


### LLM Configuration
ENABLE_LLM_CACHE=true
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=7200
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0
### Max concurrency requests of LLM
MAX_ASYNC=1
### MAX_TOKENS: max tokens send to LLM for entity relation summaries (less than context size of the model)
### MAX_TOKENS: set as num_ctx option for Ollama by API Server
MAX_TOKENS=16384


# Agno Storage Configuration

# USER CONFIGURATION
USER_ID=Eric

# Agno Storage Configuration
# Storage backend: "weaviate" or "agno"
STORAGE_BACKEND=agno
AGNO_HOME=${DATA_DIR}/${STORAGE_BACKEND}
AGNO_STORAGE_DIR=${DATA_DIR}/${STORAGE_BACKEND}/${USER_ID}
#AGNO_STORAGE_DIR=${_AGNO_STORAGE_DIR}

# Knowledge directory for Agno
AGNO_KNOWLEDGE_DIR=${DATA_DIR}/${STORAGE_BACKEND}/${USER_ID}/knowledge

# MODEL CONTEXT SIZE OVERRIDES
# Override context sizes for specific models (optional)
# Format: MODEL_NAME_CTX_SIZE (replace : with _ and . with _)
# Examples:
QWEN3_1_7B_CTX_SIZE=32768
QWEN3_4B_CTX_SIZE=32768

# LLAMA3_1_8B_INSTRUCT_Q8_0_CTX_SIZE=65536
DEFAULT_MODEL_CTX_SIZE=32768


# Embedding model (also via Ollama)

EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BINDING=ollama
EMBEDDING_BINDING_HOST=http://host.docker.internal:11434
EMBEDDING_DIM=768
# EMBEDDING_BINDING_API_KEY=your_api_key

# Optional LightRAG REST server configuration
# LLM Binding Configuration
LLM_BINDING_HOST=http://host.docker.internal:11434

# GUI Endpoint
LIGHTRAG_SERVER=http://localhost:9621
LIGHTRAG_SERVER_URL=http://localhost:9621/webui

# LLM_BINDING_API_KEY=your_api_key

# LIGHTRAG TIMEOUT SETTINGS FOR PDF PROCESSING
LLM_TIMEOUT=7200          # 120 minutes for LLM processing (increased for remote Ollama)
EMBEDDING_TIMEOUT=3600    # 60 minutes for embedding processing (increased for remote)
PDF_CHUNK_SIZE=1024       # Reduced chunk size for better reliability
HTTP_TIMEOUT=7200         # 120 minutes for HTTP client timeout
CONNECTION_TIMEOUT=600    # 10 minutes for initial connection timeout
READ_TIMEOUT=7200         # 120 minutes for read operations
WRITE_TIMEOUT=600         # 10 minutes for write operations
POOL_TIMEOUT=600          # 10 minutes for connection pool timeout

# HTTPX CLIENT TIMEOUT SETTINGS (for Ollama client)
HTTPX_TIMEOUT=7200        # 120 minutes for httpx client operations
HTTPX_CONNECT_TIMEOUT=600 # 10 minutes for httpx connection timeout
HTTPX_READ_TIMEOUT=7200   # 120 minutes for httpx read timeout
HTTPX_WRITE_TIMEOUT=600   # 10 minutes for httpx write timeout
HTTPX_POOL_TIMEOUT=600    # 10 minutes for httpx pool timeout

# OLLAMA CLIENT SPECIFIC SETTINGS
OLLAMA_TIMEOUT=7200       # 120 minutes for Ollama operations
OLLAMA_KEEP_ALIVE=3600    # Keep model loaded for 1 hour
OLLAMA_NUM_PREDICT=16384  # Maximum tokens to generate
OLLAMA_TEMPERATURE=0.1    # Lower temperature for more consistent processing

# EXTENDED TIMEOUT SETTINGS FOR LARGE DOCUMENT PROCESSING
# These settings are specifically for handling documents > 8k characters

# HTTP Client Timeouts (in seconds)
HTTPX_TIMEOUT=14400          # 4 hours total timeout
HTTPX_CONNECT_TIMEOUT=1200   # 20 minutes connection timeout
HTTPX_READ_TIMEOUT=14400     # 4 hours read timeout
HTTPX_WRITE_TIMEOUT=1200     # 20 minutes write timeout
HTTPX_POOL_TIMEOUT=1200      # 20 minutes pool timeout

# Ollama Client Timeouts
OLLAMA_TIMEOUT=14400         # 4 hours for Ollama operations
OLLAMA_REQUEST_TIMEOUT=14400 # 4 hours for individual requests
OLLAMA_KEEP_ALIVE=7200       # Keep model loaded for 2 hours
OLLAMA_NUM_PREDICT=32768     # Increased token limit
OLLAMA_TEMPERATURE=0.1       # Consistent processing

# LightRAG Processing Timeouts
LLM_TIMEOUT=14400            # 4 hours for LLM processing
EMBEDDING_TIMEOUT=7200       # 2 hours for embedding processing
PDF_CHUNK_SIZE=512           # Smaller chunks for better reliability
HTTP_TIMEOUT=14400           # 4 hours for HTTP operations
CONNECTION_TIMEOUT=1200      # 20 minutes for connections
READ_TIMEOUT=14400           # 4 hours for read operations
WRITE_TIMEOUT=1200           # 20 minutes for write operations
POOL_TIMEOUT=1200            # 20 minutes for pool operations

# Processing Configuration
MAX_RETRIES=10               # More retries for large documents
RETRY_DELAY=120              # 2 minutes between retries
BATCH_SIZE=1                 # Process one document at a time
MAX_CONCURRENT_REQUESTS=1    # Single threaded processing
BACKOFF_FACTOR=3.0           # Exponential backoff
ENABLE_CHUNKING=true         # Enable document chunking
CHUNK_OVERLAP=200            # Increased overlap for context

# Python HTTP Client Settings
REQUESTS_TIMEOUT=14400       # 4 hours for requests library
URLLIB3_TIMEOUT=14400        # 4 hours for urllib3
AIOHTTP_TIMEOUT=14400        # 4 hours for aiohttp

# System Level Timeouts
SOCKET_TIMEOUT=14400         # 4 hours for socket operations
TCP_KEEPALIVE=true           # Enable TCP keepalive
TCP_KEEPALIVE_IDLE=600       # 10 minutes idle before keepalive
TCP_KEEPALIVE_INTERVAL=60    # 1 minute between keepalive probes
TCP_KEEPALIVE_COUNT=9        # 9 failed probes before timeout
