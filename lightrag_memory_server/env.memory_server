# Personal AI Agent Environment Variables
# Copy this file to .env and fill in your actual API keys

### Server Configuration
LIGHTRAG_MEMORY_PORT=9622
WEBUI_TITLE="Personal Agent Memory Base"
WEBUI_DESCRIPTION="P.Agent Knowledge-Graph RAG"
WORKERS=4

# Directories for the Personal AI Agent
ROOT_DIR=/                     # Full filesystem access
HOME_DIR=/Users/egs           # User's home directory  
DATA_DIR=/Users/Shared/personal_agent_data     # Data directory for vector database
# USER CONFIGURATION
USER_ID=charlie

#OLLAMA_URL=http://tesla.local:11434
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_DOCKER_URL=http://host.docker.internal:11434

# LLM Model configuration - use actual model names, not dictionary references
#LLM_MODEL=llama3.1:8b-instruct-q8_0
LLM_MODEL=qwen3:1.7b

# Agno Storage Configuration
# Storage backend: "weaviate" or "agno"
STORAGE_BACKEND=agno
AGNO_STORAGE_DIR=${DATA_DIR}/${STORAGE_BACKEND}/${USER_ID}

# Knowledge directory for Agno
AGNO_KNOWLEDGE_DIR=${DATA_DIR}/${STORAGE_BACKEND}/${USER_ID}/knowledge


# MODEL CONTEXT SIZE OVERRIDES
# Override context sizes for specific models (optional)
# Format: MODEL_NAME_CTX_SIZE (replace : with _ and . with _)
# Examples:
QWEN3_1_7B_CTX_SIZE=16384
QWEN3_4B_CTX_SIZE=40960

# LLAMA3_1_8B_INSTRUCT_Q8_0_CTX_SIZE=65536
DEFAULT_MODEL_CTX_SIZE=12288


# Embedding model (also via Ollama)
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BINDING=ollama
EMBEDDING_BINDING_HOST=http://host.docker.internal:11434
EMBEDDING_DIM=768

# LLM Binding Configuration
LLM_BINDING_HOST=http://host.docker.internal:11434
# EMBEDDING_BINDING_API_KEY=your_api_key
# Optional LightRAG REST server configuration

# GUI Endpoint
LIGHTRAG_MEMORY_SERVER=http://localhost:9622
LIGHTRAG_MEMORY_SERVER_URL=http://localhost:9622/webui

# LLM_BINDING_API_KEY=your_api_key

# CONSOLIDATED TIMEOUT SETTINGS
# Optimized for qwen3:1.7b with realistic processing times

### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=1800
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0.1
### Max concurrency requests of LLM
MAX_ASYNC=4

# LightRAG Processing Timeouts
LLM_TIMEOUT=1800             # 30 minutes for LLM processing
EMBEDDING_TIMEOUT=900        # 15 minutes for embedding processing
HTTP_TIMEOUT=1800            # 30 minutes for HTTP client timeout
CONNECTION_TIMEOUT=300       # 5 minutes for initial connection timeout
READ_TIMEOUT=1800            # 30 minutes for read operations
WRITE_TIMEOUT=300            # 5 minutes for write operations
POOL_TIMEOUT=300             # 5 minutes for connection pool timeout

# HTTPX CLIENT TIMEOUT SETTINGS (for Ollama client)
HTTPX_TIMEOUT=1800           # 30 minutes for httpx client operations
HTTPX_CONNECT_TIMEOUT=300    # 5 minutes for httpx connection timeout
HTTPX_READ_TIMEOUT=1800      # 30 minutes for httpx read timeout
HTTPX_WRITE_TIMEOUT=300      # 5 minutes for httpx write timeout
HTTPX_POOL_TIMEOUT=300       # 5 minutes for httpx pool timeout

# CHUNK SETTINGS - Optimized for better context
CHUNK_SIZE=2048              # Larger chunks for better context (was 512)
CHUNK_OVERLAP_SIZE=256       # 12.5% overlap (was 100)
PDF_CHUNK_SIZE=2048          # Aligned with CHUNK_SIZE

# WORKER SETTINGS - Optimized for 14-core system
MAX_PARALLEL_INSERT=4        # Parallel inserts for better performance (was 1)

# OLLAMA CLIENT SETTINGS - Optimized timeouts
OLLAMA_TIMEOUT=1800          # 30 minutes for Ollama operations (was 7200)
OLLAMA_KEEP_ALIVE=1800       # Keep model loaded for 30 minutes (was 7200)
OLLAMA_NUM_PREDICT=16384     # Maximum tokens to generate
OLLAMA_TEMPERATURE=0.1       # Lower temperature for more consistent processing
OLLAMA_REQUEST_TIMEOUT=1800  # 30 minutes for individual requests (was 14400)

# ADDITIONAL TIMEOUT SETTINGS - Consolidated and optimized
REQUESTS_TIMEOUT=1800        # 30 minutes for requests library (was 14400)
URLLIB3_TIMEOUT=1800         # 30 minutes for urllib3 (was 14400)
AIOHTTP_TIMEOUT=1800         # 30 minutes for aiohttp (was 14400)
SOCKET_TIMEOUT=1800          # 30 minutes for socket operations (was 14400)

# TCP SETTINGS (from docker-compose.yml)
TCP_KEEPALIVE=true
TCP_KEEPALIVE_IDLE=600
TCP_KEEPALIVE_INTERVAL=60
TCP_KEEPALIVE_COUNT=9
